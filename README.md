# ğŸ¤ Toy Backend - Interactive Voice AI Backend

<div align="center">

![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-green.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

**Real-time voice agent backend with speech-to-speech capabilities**

[Features](#-features) â€¢ [Setup](#-quick-start) â€¢ [API](#-api-endpoints) â€¢ [Configuration](#-configuration)

</div>

---

## âœ¨ Features

- ğŸ™ï¸ **Real-time Speech-to-Text (STT)** using Cartesia's ink-whisper model
- ğŸ”Š **Real-time Text-to-Speech (TTS)** using Cartesia's sonic-3 model with speed/volume/emotion controls
- ğŸ¤– **LLM Integration** with Qwen/GLM models via DeepInfra
- ğŸ”„ **WebSocket-based Media Streaming** for real-time bidirectional audio
- âš¡ **Early Interruption Detection** using partial transcripts for instant response
- ğŸ¯ **Voice Activity Detection (VAD)** with Silero VAD
- ğŸŒ **Multi-language Support** (currently configured for Hindi)
- ğŸ“Š **Rich Logging** with structured logging and beautiful console output

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- API keys for:
  - Cartesia (for STT and TTS)
  - DeepInfra (for LLM)

### Installation

1. **Clone the repository:**
```bash
git clone https://github.com/inav-labs-research/toy_backend.git
cd toy_backend
```

2. **Install dependencies:**
```bash
pip install -r requirements.txt
# Or using uv:
uv pip install -r requirements.txt
```

3. **Configure `config.json`:**
   - Add your Cartesia API key for STT and TTS
   - Add your DeepInfra API key for LLM
   - Configure voice ID and language settings

4. **Run the server:**
```bash
python main.py
# Or using uv:
uv run main.py
```

The server will start on **port 5050**.

## ğŸ“¡ API Endpoints

### WebSocket

- **`/api/media-stream?agent_id=shinchan`**
  - Real-time bidirectional audio streaming
  - Supports interruption signals
  - Sends LLM text and user transcripts in real-time

### REST

- **`GET /`** - Root endpoint with API information
- **`GET /health`** - Health check endpoint

## ğŸ¤– Agents

Agents are configured in `agents.json`. The default agent is **"shinchan"**.

### Current Agent: Shinchan

- **Language**: Hindi (à¤¹à¤¿à¤‚à¤¦à¥€)
- **Voice**: Cartesia voice with speed control (0.85x)
- **Personality**: Friendly companion for children with safety guardrails

## âš™ï¸ Configuration

### `config.json` Structure

```json
{
  "models": {
    "llm_model": {
      "model_provider": "qwen",
      "model_name": "zai-org/GLM-4.5",
      "api_base": "https://api.deepinfra.com/v1/openai"
    },
    "tts_model": {
      "model_provider": "cartesia",
      "model_name": "sonic-3",
      "default_voice": "d05d32ab-146f-4ddf-8000-24d3c70fa1de",
      "language": "hi",
      "speed": 0.85
    },
    "cartesia_stt": {
      "model_provider": "cartesia",
      "model_name": "ink-whisper",
      "language": "hi"
    }
  }
}
```

### Key Settings

- **TTS Speed**: `0.6` to `1.5` (default: `0.85` for slightly slower speech)
- **TTS Volume**: `0.5` to `2.0` (default: `1.0`)
- **TTS Emotion**: Optional emotion guidance (e.g., "excited", "calm", "neutral")
- **STT Language**: Configured language for transcription
- **Interruption Sensitivity**: `max_interruptions` (default: 25)

## ğŸ—ï¸ Architecture

```
toy_backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/                    # FastAPI endpoints
â”‚   â”œâ”€â”€ agents/                 # Agent configuration loader
â”‚   â”œâ”€â”€ factories/              # Handler factories
â”‚   â”œâ”€â”€ media_stream_handler/   # WebSocket stream handlers
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ language_models/    # LLM clients (Qwen, Gemini, OpenAI)
â”‚   â”‚   â””â”€â”€ stt_models/         # STT clients (Cartesia, Soniox)
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ handlers/           # Voice handlers with interruption support
â”‚       â”œâ”€â”€ inferencing_handlers/  # Speech-to-speech inference
â”‚       â”œâ”€â”€ speech_processor/   # VAD and EOS detection
â”‚       â””â”€â”€ text_to_speech/     # TTS processors (Cartesia)
â”œâ”€â”€ agents.json                 # Agent configurations
â”œâ”€â”€ config.json                 # System configuration
â””â”€â”€ main.py                     # Application entry point
```

## ğŸ”§ Key Features Explained

### Early Interruption Detection

The system uses **partial transcripts** from Cartesia STT to detect when users start speaking, allowing TTS to stop **immediately** before the final transcript arrives. This provides near-instant interruption response.

### Real-time Text Streaming

LLM-generated text is streamed to the frontend **instantly** as tokens are generated, providing real-time visual feedback alongside audio.

### Audio Visualization

The frontend includes a beautiful audio visualizer with animated bars that respond to both microphone input and TTS output audio.

## ğŸ“ Development

### Running Tests

```bash
# Add test commands here when tests are added
```

### Code Structure

- **Async/Await**: Fully async implementation for optimal performance
- **Event-driven**: Callback-based transcript processing
- **Type Hints**: Full type annotations for better IDE support

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ™ Acknowledgments

- [Cartesia](https://cartesia.ai) for STT and TTS services
- [DeepInfra](https://deepinfra.com) for LLM hosting
- [FastAPI](https://fastapi.tiangolo.com) for the web framework

---

<div align="center">

**Built with â¤ï¸ by inav-labs-research**

[Report Bug](https://github.com/inav-labs-research/toy_backend/issues) â€¢ [Request Feature](https://github.com/inav-labs-research/toy_backend/issues)

</div>
